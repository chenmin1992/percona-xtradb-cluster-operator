#!/bin/bash

exec 5>&2
BASH_XTRACEFD="5"

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | tr '[:upper:]' '[:lower:]')}
API="pxc.percona.com/v1-2-0"
IMAGE=${IMAGE:-"perconalab/percona-xtradb-cluster-operator:${GIT_BRANCH}"}
IMAGE_PXC=${IMAGE_PXC:-"perconalab/percona-xtradb-cluster-operator:master-pxc"}
IMAGE_PMM=${IMAGE_PMM:-"perconalab/percona-xtradb-cluster-operator:master-pmm"}
IMAGE_PROXY=${IMAGE_PROXY:-"perconalab/percona-xtradb-cluster-operator:master-proxysql"}
IMAGE_BACKUP=${IMAGE_BACKUP:-"perconalab/percona-xtradb-cluster-operator:master-backup"}
tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)
if oc projects 2>&1 | egrep -q 'You have access to the following projects|You are not a member of any projects|You have one project on this server'; then
    OPENSHIFT=1
fi


create_namespace() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete project "$namespace" && sleep 40 || :
        oc new-project "$namespace"
        oc project "$namespace"
        oc adm policy add-scc-to-user hostaccess -z default || :
    else
        kubectl_bin delete namespace "$namespace" || :
        wait_for_delete "namespace/$namespace"
        kubectl_bin create namespace "$namespace"
        kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
    fi
}

get_operator_pod() {
    kubectl_bin get pods \
        --selector=name=percona-xtradb-cluster-operator \
        -o 'jsonpath={.items[].metadata.name}'
}

wait_pod() {
    local pod=$1

    set +o xtrace
    retry=0
    echo -n $pod
    #until kubectl_bin get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep 'Running'; do
    until kubectl_bin get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 480 ]; then
            kubectl_bin describe pod/$pod
            kubectl_bin logs $pod
            kubectl_bin logs $(get_operator_pod)
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

wait_backup() {
    local backup=$1

    set +o xtrace
    retry=0
    echo -n $backup
    until kubectl_bin get pxc-backup/$backup -o jsonpath='{.status.state}' 2>/dev/null | grep 'Succeeded'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 60 ]; then
            kubectl_bin logs $(get_operator_pod)
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

wait_backup_restore() {
    local backup_name=$1

    set +o xtrace
    retry=0
    echo -n $backup_name
    until kubectl_bin get pxc-restore/$backup_name -o jsonpath='{.status.state}' 2>/dev/null | grep 'Succeeded'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 600 ]; then
            kubectl_bin logs $(get_operator_pod)
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    echo
    set -o xtrace
}

deploy_operator() {
    desc 'start operator'

    kubectl_bin apply -f ${src_dir}/deploy/crd.yaml || :
    kubectl_bin apply -f ${src_dir}/deploy/rbac.yaml

    cat ${src_dir}/deploy/operator.yaml \
        | sed -e "s^image: .*^image: ${IMAGE}^" \
        | kubectl_bin apply -f -

    sleep 2

    wait_pod $(get_operator_pod)
}

deploy_helm() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        export TILLER_NAMESPACE=tiller
        HELM_VERSION=$(helm version -c | $sed -re 's/.*SemVer:"([^"]+)".*/\1/')
        oc new-project tiller || :
        oc project tiller
        oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE="tiller" -p HELM_VERSION=$HELM_VERSION | oc apply -f -
    else
        kubectl_bin --namespace kube-system create sa tiller || :
        kubectl_bin create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller || :
        helm init --service-account tiller
        kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="kube-system"
    fi

    tiller_pod=$(
        kubectl_bin get pods \
            --selector=name=tiller \
            -o 'jsonpath={.items[].metadata.name}'
    )
    wait_pod $tiller_pod

    if [ "$OPENSHIFT" == 1 ]; then
        oc project "$namespace"
        oc policy add-role-to-user edit "system:serviceaccount:tiller:tiller"
    else
        kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
    fi
}

wait_for_running() {
    local name="$1"
    let last_pod="$(($2-1))" || :

    for i in $(seq 0 $last_pod); do
        wait_pod ${name}-${i}
    done
}

wait_for_delete() {
    local res="$1"

    set +o xtrace
    echo -n "$res - "
    retry=0
    until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 120 ]; then
            kubectl_bin logs $(get_operator_pod)
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

compare_kubectl() {
    local resource="$1"
    local postfix="$2"
    local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
    local new_result="${tmp_dir}/${resource//\//_}.yml"

    if [ "$OPENSHIFT" = 1 -a -f ${expected_result//.yml/-oc.yml} ]; then
        expected_result=${expected_result//.yml/-oc.yml}
    fi

    kubectl_bin get -o yaml ${resource} \
        | egrep -v "namespace:|uid:|resourceVersion:|selfLink:|creationTimestamp:|image:|clusterIP:|dataSource:|procMount:" \
        | egrep -v "^  storageClassName:|finalizers:|kubernetes.io/pvc-protection|volumeName:|storage-provisioner:|status: \{\}|volumeMode: Filesystem" \
        | egrep -v "percona.com/.*-hash:|control-plane.alpha.kubernetes.io/leader:|volume.kubernetes.io/selected-node:" \
        | egrep -v "healthCheckNodePort:|nodePort:" \
        | $sed -e '/^status:$/,+100500d' \
        | $sed -e '/NAMESPACE/,+1d' \
        | $sed -e '/name: suffix/,+1d' \
        > ${new_result}
    diff -u ${expected_result} ${new_result}
}

get_client_pod() {
    kubectl_bin get pods \
        --selector=name=pxc-client \
        -o 'jsonpath={.items[].metadata.name}'
}

run_mysql() {
    local command="$1"
    local uri="$2"

    client_pod=$(get_client_pod)
    wait_pod $client_pod 1>&2
    [[ ${-/x} != $- ]] && echo "+ kubectl exec -it $client_pod -- bash -c \"printf '$command\n' | mysql -sN $uri\"" >&$BASH_XTRACEFD

    set +o xtrace
    kubectl_bin exec $client_pod -- \
        bash -c "printf '$command\n' | mysql -sN $uri" 2>&1  \
        | sed -e 's/mysql: //' \
        | (grep -v 'Using a password on the command line interface can be insecure.' || :)
    set -o xtrace
}

run_mysql_local() {
    local command="$1"
    local uri="$2"
    local pod="$3"

    [[ ${-/x} != $- ]] && echo "+ kubectl exec -it $pod -- bash -c \"printf '$command\n' | mysql -sN $uri\"" >&$BASH_XTRACEFD

    set +o xtrace
    kubectl_bin exec $pod -- \
        bash -c "printf '$command\n' | mysql -sN $uri" 2>&1 \
        | sed -e 's/mysql: //' \
        | (egrep -v 'Using a password on the command line interface can be insecure.|Defaulting container name|see all of the containers in this pod' || :)
    set -o xtrace
}

compare_mysql_cmd() {
    local command_id="$1"
    local command="$2"
    local uri="$3"
    local postfix="$4"

    run_mysql "$command" "$uri" \
        > $tmp_dir/${command_id}.sql
    diff ${test_dir}/compare/${command_id}${postfix}.sql $tmp_dir/${command_id}.sql
}

get_proxy_primary() {
    local uri="$1"
    local pod="$2"
    local ip=$(run_mysql_local 'SELECT hostname FROM runtime_mysql_servers WHERE hostgroup_id=11 AND status="ONLINE";' "$uri" "$pod")

    while [ $(echo "$ip" | wc -l) != 1 ]; do
        sleep 1
        ip=$(run_mysql_local 'SELECT hostname FROM runtime_mysql_servers WHERE hostgroup_id=11 AND status="ONLINE";' "$uri" "$pod")
    done

    get_pod_name "$ip"
}

get_pod_name() {
    local ip=$1
    kubectl_bin get pods -o json | jq -r '.items[] | select(.status.podIP == "'$ip'") | .metadata.name'
}

get_pod_ip() {
    local name=$1
    kubectl_bin get pods -o json | jq -r '.items[] | select(.metadata.name == "'$name'") | .status.podIP'
}

compare_mysql_user(){
    local uri="$1"
    local postfix="$2"
    local user=$(echo $uri | sed -e 's/.*-u//; s/ .*//')

    (run_mysql "SHOW GRANTS;" "$uri" || :) \
        | sed -e "s/'10[.][0-9][^']*'//; s/'[^']*[.]internal'//" \
        > $tmp_dir/$user.sql
    diff $test_dir/compare/$user$postfix.sql $tmp_dir/$user.sql
}

compare_mysql_user_local(){
    local uri="$1"
    local pod="$2"
    local postfix="$3"
    local user=$(echo $uri | sed -e 's/.*-u//; s/ .*//')

    (run_mysql_local "SHOW GRANTS;" "$uri" "$pod" || :) \
        | sed -e "s/'10[.][0-9][^']*'//; s/'[^']*[.]internal'//" \
        > $tmp_dir/$user.sql
    diff $test_dir/compare/$user$postfix.sql $tmp_dir/$user.sql
}

get_pumba() {
    kubectl_bin get pods \
        --selector=name=pumba \
        -o 'jsonpath={.items[].metadata.name}'
}

run_pumba() {
    local cmd="$*"
    kubectl_bin exec -it "$(get_pumba)" -- /pumba -l info ${cmd}
}

deploy_cert_manager() {
    kubectl_bin create namespace cert-manager || :
    kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
    kubectl_bin apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/cert-manager.yaml --validate=false || : 2>/dev/null
}

destroy() {
    local namespace="$1"

    kubectl_bin logs $(get_operator_pod) \
        | grep -v '"level":"info"' \
        | grep -v 'the object has been modified' \
        | grep -v 'get backup status: Job.batch' \
        | $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
        | sort -u \
        | tee $tmp_dir/operator.log

    #TODO: maybe will be enabled later
    #diff $test_dir/compare/operator.log $tmp_dir/operator.log

    kubectl_bin delete pxc --all
    kubectl_bin delete pxc-backup --all
    kubectl_bin delete pxc-restore --all || :

    kubectl_bin delete -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/cert-manager.yaml 2>/dev/null || :
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete --grace-period=0 --force=true project "$namespace"
    else
        kubectl_bin delete --grace-period=0 --force=true namespace "$namespace"
    fi
    rm -rf ${tmp_dir}
}

desc() {
    set +o xtrace
    local msg="$@"
    printf "\n\n-----------------------------------------------------------------------------------\n"
    printf "$msg"
    printf "\n-----------------------------------------------------------------------------------\n\n"
    set -o xtrace
}

get_service_endpoint() {
    local service=$1

    local hostname=$(
        kubectl_bin get service/$service -o json \
            | jq '.status.loadBalancer.ingress[].hostname' \
            | sed -e 's/^"//; s/"$//;'
    )
    if [ -n "$hostname" -a "$hostname" != "null" ]; then
        echo $hostname
        return
    fi

    local ip=$(
        kubectl_bin get service/$service -o json \
            | jq '.status.loadBalancer.ingress[].ip' \
            | sed -e 's/^"//; s/"$//;'
    )
    if [ -n "$ip" -a "$ip" != "null" ]; then
        echo $ip
        return
    fi

    exit 1
}

get_metric_values() {
    local metric=$1
    local instance=$2
    local user_pass=$3
    local start=$($date -u "+%s" -d "-1 minute")
    local end=$($date -u "+%s")
    local endpoint=$(get_service_endpoint monitoring-service)

    curl -s -k "https://${user_pass}@$endpoint/graph/api/datasources/proxy/1/api/v1/query_range?query=$metric%7bcontainer_name%3d%22$instance%22%7d%20or%20$metric%7binstance%3d%22$instance%22%7d&start=$start&end=$end&step=60" \
        | jq '.data.result[0].values[][1]' \
        | grep '^"[0-9]'

}

get_qan_values() {
    local instance=$1
    local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
    local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
    local endpoint=$(get_service_endpoint monitoring-service)

    local uuid=$(
        curl -s -k "https://$endpoint/qan-api/instances?deleted=no" \
            | jq '.[] | select(.Subsystem == "mysql" and .Name == "'$instance'") | .UUID' \
            | sed -e 's/^"//; s/"$//;'
    )

    curl -s -k "https://$endpoint/qan-api/qan/profile/$uuid?begin=$start&end=$end&offset=0" \
        | jq '.Query[].Fingerprint'
}

get_qan20_values() {
    local instance=$1
    local user_pass=$2
    local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
    local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
    local endpoint=$(get_service_endpoint monitoring-service)

    cat > payload.json << EOF
{
   "columns":[
      "load",
      "num_queries",
      "query_time"
   ],
   "first_seen": false,
   "group_by": "queryid",
   "include_only_fields": [],
   "keyword": "",
   "labels": [
       {
           "key": "cluster",
           "value": ["pxc"]
   }],
   "limit": 10,
   "offset": 0,
   "order_by": "-load",
   "main_metric": "load",
   "period_start_from": "$($date -u -d '-12 hour' '+%Y-%m-%dT%H:%M:%S%:z')",
   "period_start_to": "$($date -u '+%Y-%m-%dT%H:%M:%S%:z')"
}
EOF

    curl -s -k -XPOST -d @payload.json "https://${user_pass}@$endpoint/v0/qan/GetReport" \
        | jq '.rows[].fingerprint'
    rm -f payload.json
}

cat_config() {
    cat "$1" \
        | $sed -e "s#apiVersion: pxc.percona.com/v.*\$#apiVersion: $API#" \
        | $sed -e "s#image:.*-pxc\$#image: $IMAGE_PXC#" \
        | $sed -e "s#image:.*-pmm\$#image: $IMAGE_PMM#" \
        | $sed -e "s#image:.*-backup\$#image: $IMAGE_BACKUP#" \
        | $sed -e "s#image:.*-proxysql\$#image: $IMAGE_PROXY#"
}

apply_config() {
    cat_config "$1" \
        | kubectl_bin apply -f -
}

spinup_pxc() {
    local cluster=$1
    local config=$2
    local size="${3:-3}"
    local sleep="${4:-10}"

    desc 'create first PXC cluster'
    kubectl_bin apply \
        -f $conf_dir/secrets.yml

    apply_config "$conf_dir/client.yml"
    apply_config "$config"

    desc 'check if all 3 Pods started'
    wait_for_running "$cluster-proxysql" 1
    wait_for_running "$cluster-pxc" "$size"
    sleep $sleep

    desc 'write data'
    run_mysql \
        'CREATE DATABASE IF NOT EXISTS myApp; use myApp; CREATE TABLE IF NOT EXISTS myApp (id int PRIMARY KEY);' \
        "-h $cluster-proxysql -uroot -proot_password"
    run_mysql \
        'INSERT myApp.myApp (id) VALUES (100500)' \
        "-h $cluster-proxysql -uroot -proot_password"

    for i in $(seq 0 $(($size-1))); do
        compare_mysql_cmd "select-1" "SELECT * from myApp.myApp;" "-h $cluster-pxc-$i.$cluster-pxc -uroot -proot_password"
    done
}

kubectl_bin() {
    local LAST_OUT="$(mktemp)"
    local LAST_ERR="$(mktemp)"
    local exit_status=0
    for i in $(seq 0 2); do
        kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
        exit_status=$?
        [[ ${-/x} != $- ]] && echo "--- $i stdout" | cat - "$LAST_OUT" >&$BASH_XTRACEFD
        [[ ${-/x} != $- ]] && echo "--- $i stderr" | cat - "$LAST_ERR" >&$BASH_XTRACEFD
        if [[ ${exit_status} != 0 ]]; then
            sleep "$((timeout * i))"
        else
            cat "$LAST_OUT"
            cat "$LAST_ERR" >&2
            rm "$LAST_OUT" "$LAST_ERR"
            return ${exit_status}
        fi
    done
    cat "$LAST_OUT"
    cat "$LAST_ERR" >&2
    rm "$LAST_OUT" "$LAST_ERR"
    return ${exit_status}
}
